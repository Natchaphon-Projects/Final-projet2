import shap
import pandas as pd
import pickle
import numpy as np
from fastapi import APIRouter
import joblib
import json
from joblib import load
from sklearn.preprocessing import LabelEncoder
import matplotlib.pyplot as plt
from scipy.stats import mode

router = APIRouter()

# ‚úÖ ‡πÇ‡∏´‡∏•‡∏î model, columns, mapping ‡πÅ‡∏•‡∏∞ CSV
try:
    model = joblib.load("src/model/best_model.joblib")
    with open("src/model/columns.pkl", "rb") as f:
        feature_columns = pickle.load(f)
    with open("src/model/label_mappings.json", "r", encoding="utf-8") as f:
        label_mappings_json = json.load(f)    
    with open("src/model/mapping.pkl", "rb") as f:
        label_mapping = pickle.load(f)
 
    df = pd.read_csv("src/data/Real_Child_malnutrition.csv")
    scaler = load("src/model/scaler.joblib")
    print("‚úÖ scaler type:", type(scaler))

    df["Status_personal"] = df["Status_personal"].astype(str).str.strip()
    print("‚úÖ Loaded model and dataset successfully")
except Exception as e:
    print("‚ùå ERROR while loading model or data:", e)
    raise e

@router.get("/only/normal")
def explain_normal_standard():
    try:
        X = df[feature_columns].copy()

        for col in feature_columns:
            dtype = X[col].dtype

            if dtype == "object":
                try:
                    if col in label_mappings_json:
                        mapping = label_mappings_json[col]["mapping"]
                        X[col] = X[col].astype(str).str.lower().map(mapping)
                    else:
                        print(f"‚ö†Ô∏è ‡∏Ç‡πâ‡∏≤‡∏° {col} ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡πÑ‡∏°‡πà‡∏°‡∏µ mapping")
                        continue  # ‚úÖ ‡∏Ç‡πâ‡∏≤‡∏°‡πÑ‡∏õ‡∏¢‡∏±‡∏á column ‡∏ñ‡∏±‡∏î‡πÑ‡∏õ
                except Exception as e:
                    print(f"‚ö†Ô∏è ‡πÅ‡∏õ‡∏•‡∏á {col} ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ: {e} ‚Üí ‡∏Ç‡πâ‡∏≤‡∏°")
                    continue  # ‚úÖ ‡∏Ç‡πâ‡∏≤‡∏°‡πÑ‡∏õ‡∏¢‡∏±‡∏á column ‡∏ñ‡∏±‡∏î‡πÑ‡∏õ
                   
            else:
                # ‚úÖ ‡πÅ‡∏õ‡∏•‡∏á boolean ‡πÅ‡∏•‡∏∞ numeric ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô int ‡∏Å‡πà‡∏≠‡∏ô scale
                try:
                    X[col] = X[col].astype(int)
                except Exception as e:
                    print(f"‚ö†Ô∏è ‡πÅ‡∏õ‡∏•‡∏á {col} ‡πÄ‡∏õ‡πá‡∏ô int ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ: {e} ‚Üí ‡∏Ç‡πâ‡∏≤‡∏°")

        # ‚úÖ ‡πÅ‡∏õ‡∏•‡∏á‡∏ó‡∏∏‡∏Å column ‡πÄ‡∏õ‡πá‡∏ô float ‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏Ç‡πâ‡∏≤ scaler
        X = X.astype(float)
        

        # ‚úÖ ‡∏ó‡∏≥ StandardScaler
        X_scaled = pd.DataFrame(scaler.transform(X), columns=feature_columns)

        # ‚úÖ ‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏î‡πâ‡∏ß‡∏¢ SHAP
        explainer = shap.TreeExplainer(model)
        shap_values = explainer(X_scaled, check_additivity=False)
    
        shap_values_class0 = shap.Explanation(
            values=shap_values.values[:, :, 0],
            base_values=shap_values.base_values[:, 0],
            data=shap_values.data,
            feature_names=shap_values.feature_names
        )
        
        # ‚úÖ ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤ top feature
        top_shap_per_feature = []
        for i, feature in enumerate(shap_values_class0.feature_names):
            shap_col = shap_values_class0.values[:, i]
            max_idx = np.argmax(shap_col)
            scaled_row = shap_values_class0.data[max_idx].reshape(1, -1)
            original_row = scaler.inverse_transform(scaled_row)[0]

            top_shap_per_feature.append({
                "feature": feature,
                "max_shap": float(shap_col[max_idx]),
                "real_value": float(scaled_row[0][i]),
                "real_value_original": int(original_row[i] + 0.5),
                "row_index": int(max_idx)
            })

        return {
            "shap_class": 0,
            "top_features": top_shap_per_feature
        }

    except Exception as e:
        import traceback
        traceback.print_exc()
        return {"error": str(e)}




@router.get("/global/{status}")
def explain_global(status: str):
    try:
        print("üìå Status ‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏ö‡∏°‡∏≤:", status)

        # ‚úÖ ‡πÇ‡∏´‡∏•‡∏î mapping ‡∏Ç‡∏≠‡∏á Status_personal
        status_mapping = label_mapping.get("Status_personal", {})
        print("‚úÖ Mapping ‡∏Ç‡∏≠‡∏á Status_personal:", status_mapping)

        # ‚úÖ ‡πÅ‡∏õ‡∏•‡∏á status ‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏ö‡πÄ‡∏Ç‡πâ‡∏≤‡∏°‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÉ‡∏ô model (int)
        class_personal = status.strip()
        if class_personal in status_mapping:
            pred_class = int(status_mapping[class_personal])
            print("‚úÖ class_personal:", class_personal)
            print("‚úÖ ‡πÅ‡∏õ‡∏•‡∏á‡πÅ‡∏•‡πâ‡∏ß‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡πà‡∏≤:", pred_class)
        else:
            return {
                "error": f"‡∏Ñ‡πà‡∏≤ Status_personal '{class_personal}' ‡πÑ‡∏°‡πà‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô label_mapping",            
            }


        X = df[feature_columns].copy()

        for col in feature_columns:
            dtype = X[col].dtype

            if dtype == "object":
                try:
                    if col in label_mappings_json:
                        mapping = label_mappings_json[col]["mapping"]
                        X[col] = X[col].astype(str).str.lower().map(mapping)
                    else:
                        print(f"‚ö†Ô∏è ‡∏Ç‡πâ‡∏≤‡∏° {col} ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡πÑ‡∏°‡πà‡∏°‡∏µ mapping")
                        continue  # ‚úÖ ‡∏Ç‡πâ‡∏≤‡∏°‡πÑ‡∏õ‡∏¢‡∏±‡∏á column ‡∏ñ‡∏±‡∏î‡πÑ‡∏õ
                except Exception as e:
                    print(f"‚ö†Ô∏è ‡πÅ‡∏õ‡∏•‡∏á {col} ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ: {e} ‚Üí ‡∏Ç‡πâ‡∏≤‡∏°")
                    continue  # ‚úÖ ‡∏Ç‡πâ‡∏≤‡∏°‡πÑ‡∏õ‡∏¢‡∏±‡∏á column ‡∏ñ‡∏±‡∏î‡πÑ‡∏õ
                   
            else:
                # ‚úÖ ‡πÅ‡∏õ‡∏•‡∏á boolean ‡πÅ‡∏•‡∏∞ numeric ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô int ‡∏Å‡πà‡∏≠‡∏ô scale
                try:
                    X[col] = X[col].astype(int)
                except Exception as e:
                    print(f"‚ö†Ô∏è ‡πÅ‡∏õ‡∏•‡∏á {col} ‡πÄ‡∏õ‡πá‡∏ô int ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ: {e} ‚Üí ‡∏Ç‡πâ‡∏≤‡∏°")

        # ‚úÖ ‡πÅ‡∏õ‡∏•‡∏á‡∏ó‡∏∏‡∏Å column ‡πÄ‡∏õ‡πá‡∏ô float ‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏Ç‡πâ‡∏≤ scaler
        X = X.astype(float)
        

        # ‚úÖ ‡∏ó‡∏≥ StandardScaler
        X_scaled = pd.DataFrame(scaler.transform(X), columns=feature_columns)

         # ‚úÖ ‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏î‡πâ‡∏ß‡∏¢ SHAP
        explainer = shap.TreeExplainer(model)
        shap_values = explainer(X_scaled, check_additivity=False)

        shap_values_status = shap.Explanation(
            values=shap_values.values[:, :, pred_class],
            base_values=shap_values.base_values[:, pred_class],
            data=shap_values.data,
            feature_names=shap_values.feature_names
        )
        
        rows = []
        shap_values_array = shap_values_status.values
        feature_values_array = shap_values_status.data
        feature_names = shap_values_status.feature_names

        for i, name in enumerate(feature_names):
            shap_vals = shap_values_array[:, i]
            feature_vals = feature_values_array[:, i]

            # ‚úÖ ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Q1 ‡πÅ‡∏•‡∏∞ Q3
            q1 = np.percentile(shap_vals, 25)
            q3 = np.percentile(shap_vals, 75)

            # ‚úÖ ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô IQR
            mask = (shap_vals >= q1) & (shap_vals <= q3)
            filtered_feature_vals = feature_vals[mask]
            filtered_shap_vals = shap_vals[mask]

            if len(filtered_feature_vals) == 0:
                rows.append({
                    "feature": name,
                    "mode_in_IQR": None,
                    "mean_shap_at_mode": None,
                    "count_at_mode": 0
                })
                continue

            # ‚úÖ ‡∏´‡∏≤‡∏Ñ‡πà‡∏≤ mode
            mode_result = mode(filtered_feature_vals, keepdims=False)
            mode_val = mode_result.mode
            # print(mode_val)
            if isinstance(mode_val, np.ndarray):
                mode_val = mode_val.item() if mode_val.size > 0 else None

            count_mode = np.sum(filtered_feature_vals == mode_val)
            # print(count_mode)
            # ‚úÖ ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì mean SHAP ‡∏Ç‡∏≠‡∏á‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö mode
            shap_in_mode = filtered_shap_vals[filtered_feature_vals == mode_val]
            mean_shap_in_mode = shap_in_mode.mean() if len(shap_in_mode) > 0 else None

            # ‚úÖ ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° array ‡∏Ç‡∏ô‡∏≤‡∏î 1xN ‡πÄ‡∏û‡∏∑‡πà‡∏≠ inverse ‡∏Ñ‡πà‡∏≤ mode_val
            scaled_input = np.zeros((1, len(feature_names)))
            if mode_val is not None:
                scaled_input[0, i] = mode_val
                original_input = scaler.inverse_transform(scaled_input)
                original_mode_val = original_input[0, i]
            else:
                original_mode_val = None
            print(original_mode_val)
            rows.append({
                "feature": str(name),
                "mode_in_IQR": int(original_mode_val + 0.5) ,
                "mean_shap_at_mode": float(mean_shap_in_mode) ,
                "count_at_mode": int(count_mode) 
            })
        # ‚úÖ ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏à‡∏≤‡∏Å SHAP ‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î
        rows = sorted(rows, key=lambda x: (x["mean_shap_at_mode"]), reverse=True)

        return {
            "shap_class": pred_class,
            "summary_by_feature": rows
        }

    except Exception as e:
        import traceback
        traceback.print_exc()
        return {"error": str(e)}


@router.get("/global/most/{status}")
def explain_global_most(status: str):
    try:
        print("üìå Status ‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏ö‡∏°‡∏≤:", status)

        # ‚úÖ ‡πÇ‡∏´‡∏•‡∏î mapping ‡∏Ç‡∏≠‡∏á Status_personal
        status_mapping = label_mapping.get("Status_personal", {})
        print("‚úÖ Mapping ‡∏Ç‡∏≠‡∏á Status_personal:", status_mapping)

        # ‚úÖ ‡πÅ‡∏õ‡∏•‡∏á status ‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏ö‡πÄ‡∏Ç‡πâ‡∏≤‡∏°‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÉ‡∏ô model (int)
        class_personal = status.strip()
        if class_personal in status_mapping:
            pred_class = int(status_mapping[class_personal])
            print("‚úÖ class_personal:", class_personal)
            print("‚úÖ ‡πÅ‡∏õ‡∏•‡∏á‡πÅ‡∏•‡πâ‡∏ß‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡πà‡∏≤:", pred_class)
        else:
            return {
                "error": f"‡∏Ñ‡πà‡∏≤ Status_personal '{class_personal}' ‡πÑ‡∏°‡πà‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô label_mapping",            
            }


        X = df[feature_columns].copy()

        for col in feature_columns:
            dtype = X[col].dtype

            if dtype == "object":
                try:
                    if col in label_mappings_json:
                        mapping = label_mappings_json[col]["mapping"]
                        X[col] = X[col].astype(str).str.lower().map(mapping)
                    else:
                        print(f"‚ö†Ô∏è ‡∏Ç‡πâ‡∏≤‡∏° {col} ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡πÑ‡∏°‡πà‡∏°‡∏µ mapping")
                        continue  # ‚úÖ ‡∏Ç‡πâ‡∏≤‡∏°‡πÑ‡∏õ‡∏¢‡∏±‡∏á column ‡∏ñ‡∏±‡∏î‡πÑ‡∏õ
                except Exception as e:
                    print(f"‚ö†Ô∏è ‡πÅ‡∏õ‡∏•‡∏á {col} ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ: {e} ‚Üí ‡∏Ç‡πâ‡∏≤‡∏°")
                    continue  # ‚úÖ ‡∏Ç‡πâ‡∏≤‡∏°‡πÑ‡∏õ‡∏¢‡∏±‡∏á column ‡∏ñ‡∏±‡∏î‡πÑ‡∏õ
                   
            else:
                # ‚úÖ ‡πÅ‡∏õ‡∏•‡∏á boolean ‡πÅ‡∏•‡∏∞ numeric ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô int ‡∏Å‡πà‡∏≠‡∏ô scale
                try:
                    X[col] = X[col].astype(int)
                except Exception as e:
                    print(f"‚ö†Ô∏è ‡πÅ‡∏õ‡∏•‡∏á {col} ‡πÄ‡∏õ‡πá‡∏ô int ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ: {e} ‚Üí ‡∏Ç‡πâ‡∏≤‡∏°")

        # ‚úÖ ‡πÅ‡∏õ‡∏•‡∏á‡∏ó‡∏∏‡∏Å column ‡πÄ‡∏õ‡πá‡∏ô float ‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏Ç‡πâ‡∏≤ scaler
        X = X.astype(float)
        

        # ‚úÖ ‡∏ó‡∏≥ StandardScaler
        X_scaled = pd.DataFrame(scaler.transform(X), columns=feature_columns)

        # ‚úÖ ‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏î‡πâ‡∏ß‡∏¢ SHAP
        explainer = shap.TreeExplainer(model)
        shap_values = explainer(X_scaled, check_additivity=False)
    
        shap_values_most = shap.Explanation(
            values=shap_values.values[:, :, pred_class],
            base_values=shap_values.base_values[:, pred_class],
            data=shap_values.data,
            feature_names=shap_values.feature_names
        )
        
        # ‚úÖ ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤ top feature
        top_shap_per_feature = []
        for i, feature in enumerate(shap_values_most.feature_names):
            shap_col = shap_values_most.values[:, i]
            max_idx = np.argmax(shap_col)
            scaled_row = shap_values_most.data[max_idx].reshape(1, -1)
            original_row = scaler.inverse_transform(scaled_row)[0]

            top_shap_per_feature.append({
                "feature": feature,
                "max_shap": float(shap_col[max_idx]),
                "real_value": float(scaled_row[0][i]),
                "real_value_original": int(original_row[i] + 0.5),
                "row_index": int(max_idx)
            })

        return {
            "shap_class": pred_class,
            "top_features": top_shap_per_feature
        }
        

    except Exception as e:
        import traceback
        traceback.print_exc()
        return {"error": str(e)}
    
__all__ = ["router"]