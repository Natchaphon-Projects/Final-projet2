import shap
import pandas as pd
import pickle
import numpy as np
from fastapi import APIRouter, Query
import joblib
import json
from joblib import load
from sklearn.preprocessing import LabelEncoder
import matplotlib.pyplot as plt
import mysql.connector

router = APIRouter()


# ‚úÖ ‡πÇ‡∏´‡∏•‡∏î model ‡πÅ‡∏•‡∏∞ metadata
try:
    model = joblib.load("src/model/best_model.joblib")
    with open("src/model/columns.pkl", "rb") as f:
        feature_columns = pickle.load(f)
    with open("src/model/label_mappings.json", "r", encoding="utf-8") as f:
        label_mappings_json = json.load(f)    
    with open("src/model/mapping.pkl", "rb") as f:
        label_mapping = pickle.load(f)
    scaler = load("src/model/scaler.joblib")

except Exception as e:
    print("‚ùå ERROR while loading model or data:", e)
    raise e

# ‚úÖ ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡∏à‡∏≤‡∏Å MySQL
def get_patient_data_from_db(patient_id, created_at_str):
    conn = mysql.connector.connect(
        host="localhost",
        user="root",
        password="root",
        database="child_malnutrition"
    )
    cursor = conn.cursor(dictionary=True)
    query = """
        SELECT * FROM prediction
        WHERE patient_id = %s AND created_at = %s
        LIMIT 1
    """
    cursor.execute(query, (patient_id, created_at_str))
    result = cursor.fetchone()
    cursor.close()
    conn.close()

    return result  # ‚úÖ ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á pop ‡πÉ‡∏î ‡πÜ ‡∏à‡∏∞‡πÑ‡∏î‡πâ‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ‡∏ó‡∏±‡πâ‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏•‡∏∞‡πÄ‡∏ß‡∏•‡∏≤

# ‚úÖ Endpoint ‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢ SHAP ‡πÅ‡∏ö‡∏ö local
@router.get("/shap/local/{patient_id}")
def explain_local(patient_id: int, created_at: str = Query(...)):
    try:
        print(f"üìå ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡∏≠‡∏á‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢ ID: {patient_id}")
        patient_data = get_patient_data_from_db(patient_id, created_at)
        print(patient_data)
        if patient_data is None:
            return {"error": "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢"}

        created_at = str(patient_data["created_at"])  # ‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏ß‡πâ‡∏™‡πà‡∏á‡∏Å‡∏•‡∏±‡∏ö
        class_personal = str(patient_data["Status_personal"]).strip()
        print("‚úÖ class_personal:", class_personal)
        print("‚úÖ label_mapping keys:", label_mapping.keys())  # ‡∏à‡∏∞‡πÄ‡∏´‡πá‡∏ô ['Status_personal']
        status_mapping = label_mapping.get("Status_personal", {})

        # ‚úÖ ‡πÅ‡∏õ‡∏•‡∏á‡∏à‡∏≤‡∏Å string ‡πÄ‡∏õ‡πá‡∏ô index ‡∏î‡πâ‡∏ß‡∏¢ label_mapping
        print(label_mapping)
        if class_personal in status_mapping:
            pred_class = status_mapping[class_personal]
        else:
            return {"error": f"Status_personal '{class_personal}' ‡πÑ‡∏°‡πà‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô label_mapping"}
        print(pred_class)
        patient_data_filtered = {k: patient_data[k] for k in feature_columns}
        patient_df = pd.DataFrame([patient_data_filtered])

        for col in feature_columns:
            dtype = patient_df[col].dtype

            try:
                # ‚úÖ ‡πÅ‡∏õ‡∏•‡∏á "True"/"False" string ‚Üí 1/0 ‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏Ç‡πâ‡∏≤ model
                patient_df[col] = patient_df[col].astype(str).str.strip().str.lower()

                if patient_df[col].isin(["true", "false"]).all():
                    patient_df[col] = patient_df[col].map({"true": 1, "false": 0})

                elif col in label_mappings_json:
                    mapping = label_mappings_json[col]["mapping"]
                    patient_df[col] = patient_df[col].map(mapping)

                else:
                    patient_df[col] = pd.to_numeric(patient_df[col], errors="coerce").fillna(0).astype(int)

            except Exception as e:
                print(f"‚ö†Ô∏è ‡πÅ‡∏õ‡∏•‡∏á {col} ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ: {e} ‚Üí ‡∏Ç‡πâ‡∏≤‡∏°")
        print(patient_df)

        patient_df = patient_df.astype(float)
        X_scaled = pd.DataFrame(scaler.transform(patient_df), columns=feature_columns)

        # ‚úÖ ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å TreeExplainer
        explainer = shap.TreeExplainer(model)
        shap_values = explainer(X_scaled, check_additivity=False)

        print(X_scaled)
        print("‚úÖ shap_values.shape:", shap_values.shape)
        print("‚úÖ type:", type(shap_values))

        # üîÅ ‡∏™‡∏•‡∏±‡∏ö shape ‚Üí ‡∏à‡∏≤‡∏Å (1, 42, 6) ‡πÄ‡∏õ‡πá‡∏ô (1, 6, 42)
        shap_values.values = shap_values.values.transpose(0, 2, 1)
        shap_values.base_values = shap_values.base_values.transpose(0, 1)

        # ‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á shap explanation object ‡∏Ç‡∏≠‡∏á‡πÄ‡∏î‡πá‡∏Å‡∏Ñ‡∏ô‡∏ô‡∏±‡πâ‡∏ô
        shap_row = shap.Explanation(
            values=shap_values.values[0, pred_class],  # ‚Üê class ‡πÄ‡∏î‡πá‡∏Å‡∏Ñ‡∏ô‡∏ô‡∏±‡πâ‡∏ô ‡πÄ‡∏ä‡πà‡∏ô SAM = 3
            base_values=shap_values.base_values[0, pred_class],
            data=shap_values.data[0],
            feature_names=shap_values.feature_names
        )

        print("‚úÖ values shape:", shap_values.values.shape)
        print("‚úÖ base_values shape:", shap_values.base_values.shape)
        print("‚úÖ feature_names:", shap_values.feature_names)
        # ‚úÖ ‡πÅ‡∏™‡∏î‡∏á waterfall plot

        

        # ‚úÖ ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡∏à‡∏≤‡∏Å StandardScaler ‚Üí ‡∏Ñ‡πà‡∏≤‡πÄ‡∏î‡∏¥‡∏°
        # scaler.mean_ ‡πÅ‡∏•‡∏∞ scaler.scale_ ‡∏°‡∏µ‡∏Ñ‡πà‡∏≤‡πÄ‡∏ó‡πà‡∏≤‡∏Å‡∏±‡∏ö‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡πÅ‡∏•‡∏∞‡∏™‡πÄ‡∏Å‡∏•‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏ï‡∏≠‡∏ô train

        # ‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏ñ‡∏ß‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å scale
        scaled_row = X_scaled.values[0].reshape(1, -1)

        # ‚úÖ ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡∏à‡∏≤‡∏Å scaler ‚Üí original (‡∏Å‡πà‡∏≠‡∏ô normalize)
        original_row = scaler.inverse_transform(scaled_row)[0]  # ‚Üí array 1 ‡∏°‡∏¥‡∏ï‡∏¥

        # ‚úÖ ‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• shap ‡πÅ‡∏ï‡πà‡∏•‡∏∞ feature
        top_shap_per_feature = []
        for i, feature_name in enumerate(shap_row.feature_names):
            top_shap_per_feature.append({
                "feature": feature_name,
                "value": float(scaled_row[0][i]),   # ‡∏Ñ‡πà‡∏≤‡∏´‡∏•‡∏±‡∏á preprocess (input model)
                "real_value": int(original_row[i] + 0.5),               # ‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà invert ‡∏à‡∏≤‡∏Å StandardScaler
                "shap": float(shap_row.values[i])                   # SHAP value
            })
        top_shap_per_feature = sorted(top_shap_per_feature, key=lambda x: x["shap"], reverse=True)
        return {
            "shap_class": int(pred_class),
            "top_features": top_shap_per_feature
        }

    except Exception as e:
        import traceback
        traceback.print_exc()
        return {"error": str(e)}